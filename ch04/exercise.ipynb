{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matplotlib version: 3.10.1\n",
      "torch version: 2.5.1\n",
      "tiktoken version: 0.9.0\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "print(\"matplotlib version:\", version(\"matplotlib\"))\n",
    "print(\"torch version:\", version(\"torch\"))\n",
    "print(\"tiktoken version:\", version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4.1\n",
    "\n",
    "feed forward와 attention module의 파라미터 수를 구하라"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ch04 import FeedForward, MultiHeadAttention, GPT_124M_CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "def count_parameter(m: nn.Module):\n",
    "    return sum([p.numel() for p in m.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff = FeedForward(cfg=GPT_124M_CONFIG)\n",
    "attn = MultiHeadAttention(\n",
    "    d_in=GPT_124M_CONFIG.emb_dim,\n",
    "    d_out=GPT_124M_CONFIG.emb_dim,\n",
    "    num_heads=GPT_124M_CONFIG.n_heads,\n",
    "    qkv_bias=GPT_124M_CONFIG.qkv_bias,\n",
    "    dropout=GPT_124M_CONFIG.drop_ratio,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feed Forward count: 4722432\n",
      "MHA count: 2360064\n"
     ]
    }
   ],
   "source": [
    "print(f\"Feed Forward count: {count_parameter(ff)}\")\n",
    "print(f\"MHA count: {count_parameter(attn)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4.2\n",
    "\n",
    "GPT2 medium, GPT large, GPT Xlarge 모델을 직접 만드시요.\n",
    "일단 허깅페이스 config 부터 가져오자\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ch04 import GPTConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://huggingface.co/openai-community/gpt2-medium/blob/main/config.json\n",
    "GPT_MEDIUM_CONFIG = GPTConfig(\n",
    "    vocab_size=50257,\n",
    "    max_context_length=1024, # n_position\n",
    "    emb_dim=1024,\n",
    "    n_heads=16,\n",
    "    n_layers=24,\n",
    "    drop_ratio=0.1,\n",
    "    qkv_bias=False\n",
    ")\n",
    "\n",
    "# from https://huggingface.co/openai-community/gpt2-large/blob/main/config.json\n",
    "GPT_LARGE_CONFIG = GPTConfig(\n",
    "    vocab_size=50257,\n",
    "    max_context_length=1024, # n_position\n",
    "    emb_dim=1280,\n",
    "    n_heads=20,\n",
    "    n_layers=36,\n",
    "    drop_ratio=0.1,\n",
    "    qkv_bias=False\n",
    ")\n",
    "\n",
    "# from https://huggingface.co/openai-community/gpt2-xl/blob/main/config.json\n",
    "GPT_XL_CONFIG = GPTConfig(\n",
    "    vocab_size=50257,\n",
    "    max_context_length=1024, # n_position\n",
    "    emb_dim=1600,\n",
    "    n_heads=25,\n",
    "    n_layers=48,\n",
    "    drop_ratio=0.1,\n",
    "    qkv_bias=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ch04 import GPTModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "medium = GPTModel(GPT_MEDIUM_CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기 밑에 있는 건 사이즈가 커서 실행하는 거 생략한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "large = GPTModel(GPT_LARGE_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xl = GPTModel(GPT_XL_CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4.3\n",
    "\n",
    "dropout을 사용했는데 이를 각 모듈별로 나누어서 설정해 보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class ExtendedGPTConfig:\n",
    "    vocab_size: int\n",
    "    max_context_length: int\n",
    "    emb_dim: int\n",
    "    n_heads: int\n",
    "    n_layers: int\n",
    "    drop_rate_emb: float\n",
    "    drop_rate_attn: float\n",
    "    drop_rate_shortcut: float\n",
    "    qkv_bias: bool\n",
    "    \n",
    "GPT_124M_CONFIG = ExtendedGPTConfig(\n",
    "    vocab_size=50257,\n",
    "    max_context_length=1024,\n",
    "    emb_dim=768,\n",
    "    n_heads=12,\n",
    "    n_layers=12,\n",
    "    drop_rate_emb=0.1,\n",
    "    drop_rate_attn=0.1,\n",
    "    drop_rate_shortcut=0.1,\n",
    "    qkv_bias=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ch04 import LayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg: ExtendedGPTConfig):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attn = MultiHeadAttention(\n",
    "            d_in=cfg.emb_dim,\n",
    "            d_out=cfg.emb_dim,\n",
    "            num_heads=cfg.n_heads,\n",
    "            qkv_bias=cfg.qkv_bias,\n",
    "            dropout=cfg.drop_rate_attn,\n",
    "        )\n",
    "        \n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg.emb_dim)\n",
    "        self.norm2 = LayerNorm(cfg.emb_dim)\n",
    "        self.drop_shortcut = nn.Dropout(cfg.drop_rate_shortcut)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # attention\n",
    "        short_cut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.attn(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + short_cut\n",
    "        \n",
    "        # feedforward \n",
    "        short_cut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + short_cut\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg: ExtendedGPTConfig):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.tok_emb= nn.Embedding(cfg.vocab_size, cfg.emb_dim)\n",
    "        self.pos_emb= nn.Embedding(cfg.max_context_length, cfg.emb_dim)\n",
    "        self.drop_emb = nn.Dropout(cfg.drop_rate_emb)\n",
    "        \n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg.n_layers)]\n",
    "        )\n",
    "        \n",
    "        self.final_norm = LayerNorm(cfg.emb_dim)\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg.emb_dim, cfg.vocab_size, bias=False\n",
    "        )\n",
    "        \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        _batch_size, sequence = x.shape\n",
    "        tok_embbed = self.tok_emb(x)\n",
    "        in_consecutive_idx = torch.arange(sequence, device=x.device)\n",
    "        pos_embbed = self.pos_emb(in_consecutive_idx)\n",
    "        x_embbed = tok_embbed * pos_embbed\n",
    "        \n",
    "        x_drop_out = self.drop_emb(x_embbed)\n",
    "        x = self.trf_blocks(x_drop_out)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPTModel(GPT_124M_CONFIG)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
