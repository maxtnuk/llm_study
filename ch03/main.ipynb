{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.5.1\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "print(\"torch version:\", version(\"torch\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple attention\n",
    "\n",
    "간단하게 attention을 수행하는 모델을 만들어 봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "먼저 input token을 생성을 한다.\n",
    "이때 input은 sequence는 6으로 하고 각 token 마다 길이가 3인 embedding 벡터를 생성한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 3])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 여기에 q2에 대해서 attention score를 계산한다.\n",
    "우리가 목표하는 거는 query token 2에 대해서 input으로 들어오는 prompt들에 대한 context vector를 만드는 방식이다.\n",
    "여러 방식들이 있을 수 있지만 가장 간단한 방법은 각 embedding vector를 각 query 마다 곱하는 것이다. 이를 통해 각 token 사이의 관계를 담을 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "q2 = inputs[1]\n",
    "\n",
    "attn_score_2 = torch.empty(inputs.shape[0])\n",
    "\n",
    "for i, x_i in enumerate(inputs):\n",
    "    attn_score_2[i] = torch.dot(x_i, q2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_score_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "attn_score_2에는 각 input token 별로 곱해진 tensor를 가지고 있다. \n",
    "이들 값들을 normalize할 필요가 있다. 가장 간단한 방법은 전체 합으로 나누는 방식이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
      "Sum: tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "attn_weights_2_tmp = attn_score_2 / attn_score_2.sum()\n",
    "\n",
    "print(\"Attention weights:\", attn_weights_2_tmp)\n",
    "print(\"Sum:\", attn_weights_2_tmp.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이를 좀더 training이 가능하고 extreme value들을 다루기 위해 softmax를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "def softmax_naive(x):\n",
    "    return torch.exp(x) / torch.exp(x).sum(dim=0)\n",
    "\n",
    "attn_weights_2_naive = softmax_naive(attn_score_2)\n",
    "\n",
    "print(\"Attention weights:\", attn_weights_2_naive)\n",
    "print(\"Sum:\", attn_weights_2_naive.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "attn_weights_2 = torch.softmax(attn_score_2, dim=0)\n",
    "\n",
    "print(\"Attention weights:\", attn_weights_2)\n",
    "print(\"Sum:\", attn_weights_2.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 마지막으로 attention weight를 각 input에 곱해지게 되면 context vector가 만들어진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "context_vec_2 = torch.zeros(q2.shape)\n",
    "for i,x_i in enumerate(inputs):\n",
    "    context_vec_2 += attn_weights_2[i]*x_i\n",
    "\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "지금까지는 query 2에 대해서 attention을 수행했는데 사실은 모든 query에 대해서 context vector를 만들어 내야 한다.\n",
    "query token하나를 기준으로 dot product를 하는 방식은 결국에는 matrix 연산과 같다.\n",
    "따라서 이를 matrix 연산으로 만들 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_score = inputs @ inputs.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
       "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
       "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
       "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
       "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
       "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서 주목해야 하는 거는 [6,3] @ [3,6] 으로 하여 [6,6] 모양의 행렬이 생성이 되었다.\n",
    "즉 이 과정에서 embedding 축이 서로 contraction 되어서 사라지는 걸 알 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1739, 0.1723, 0.1719, 0.1596, 0.1593, 0.1630],\n",
       "        [0.1618, 0.1787, 0.1779, 0.1595, 0.1570, 0.1650],\n",
       "        [0.1619, 0.1786, 0.1778, 0.1595, 0.1574, 0.1648],\n",
       "        [0.1628, 0.1735, 0.1730, 0.1632, 0.1600, 0.1675],\n",
       "        [0.1643, 0.1715, 0.1718, 0.1617, 0.1702, 0.1605],\n",
       "        [0.1619, 0.1753, 0.1744, 0.1625, 0.1556, 0.1704]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_score = torch.softmax(attention_score, dim = -1)\n",
    "attention_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그리고 query 축이 아닌 k축을 기준으로 softmax를 취한다.\n",
    "마지막으로 모든 query에 대한 context를 vector를 구하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4334, 0.5849, 0.5368],\n",
       "        [0.4335, 0.5948, 0.5350],\n",
       "        [0.4337, 0.5945, 0.5348],\n",
       "        [0.4315, 0.5911, 0.5321],\n",
       "        [0.4375, 0.5847, 0.5280],\n",
       "        [0.4295, 0.5945, 0.5343]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vectors = attention_score @ inputs\n",
    "context_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainable model\n",
    "\n",
    "우리는 지금까지 input의 embedding wight만을 이용해서 context vector를 구했다.\n",
    "하지만 이럴 경우에는 모델은 전적으롬 embedding weight만으로 결정이 된다.\n",
    "즉 모델만의 사고 방식을 저장할 수 있는 요소가 필요하다. \n",
    "이를 위해 모델 weight를 넣었습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리는 input에 바로 attention을 계산하는 방식으로 했다. 사실은 이를 각 역활마다 weight를 넣을 수 있다.\n",
    "query weight: token에 대한 weight\n",
    "key weight: token에 대응하는 각 token들에 대한 weight\n",
    "value weight: query key 기반해서 값으로 나타내야 값에 대한 weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2 = inputs[1] # second input element\n",
    "d_in = inputs.shape[1] # the input embedding size, d=3\n",
    "d_out = 2 # the output embedding size, d=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_key   = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4306, 1.4551])\n"
     ]
    }
   ],
   "source": [
    "query_2 = x_2 @ W_query # _2 because it's with respect to the 2nd input element\n",
    "key_2 = x_2 @ W_key \n",
    "value_2 = x_2 @ W_value\n",
    "\n",
    "print(query_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys.shape: torch.Size([6, 2])\n",
      "values.shape: torch.Size([6, 2])\n"
     ]
    }
   ],
   "source": [
    "keys = inputs @ W_key \n",
    "values = inputs @ W_value\n",
    "\n",
    "print(\"keys.shape:\", keys.shape)\n",
    "print(\"values.shape:\", values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_score_2 = query_2 @ keys.T \n",
    "attn_score_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서의 d_k는 d_out에 따른 scale을 나눈 파라미터이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\n"
     ]
    }
   ],
   "source": [
    "d_k = keys.shape[-1]\n",
    "attn_weights_2 = torch.softmax(attn_score_2 / d_k**0.5, dim=-1)\n",
    "print(attn_weights_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3061, 0.8210])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vector_2 = attn_weights_2 @ values \n",
    "context_vector_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self attention impl\n",
    "\n",
    "이제 nn.module 로 attetion module을 만들어 보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "import numpy as np\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_in, d_out, attn_dtype: torch.dtype = torch.float32):\n",
    "        super().__init__()\n",
    "        self.d_in = d_in\n",
    "        self.d_out = d_out\n",
    "        self.attn_dtype = attn_dtype\n",
    "        self.k_w = nn.Parameter(torch.rand(d_in, d_out, dtype=self.attn_dtype))\n",
    "        self.q_w = nn.Parameter(torch.rand(d_in, d_out, dtype=self.attn_dtype))\n",
    "        self.v_w = nn.Parameter(torch.rand(d_in, d_out, dtype=self.attn_dtype))\n",
    "        self.inverse_d_k = np.reciprocal(np.sqrt(d_out))\n",
    "        \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        weighted_q = x @ self.k_w \n",
    "        weighted_k = x @ self.q_w\n",
    "        attention_scores = weighted_q @ weighted_k.T \n",
    "        attention_weights = torch.softmax(attention_scores*self.inverse_d_k , dim=-1)\n",
    "        weighted_v = x @ self.v_w\n",
    "        context_vectors = attention_weights @ weighted_v\n",
    "        return context_vectors\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2996, 0.8053],\n",
      "        [0.3061, 0.8210],\n",
      "        [0.3058, 0.8203],\n",
      "        [0.2948, 0.7939],\n",
      "        [0.2927, 0.7891],\n",
      "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "sa_v1 = SelfAttention(d_in, d_out)\n",
    "print(sa_v1(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.Parameter를 바로 만드는 것 보다는 nn.Linear를 만드는 게 좀 더 좋다 왜냐하면\n",
    "weiht를 train할 때 linear 라는 힌트를 통해 학습을 할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionV2(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_in, d_out, attn_dtype: torch.dtype = torch.float32, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_in = d_in\n",
    "        self.d_out = d_out\n",
    "        self.attn_dtype = attn_dtype\n",
    "        self.k_w = nn.Linear(d_in, d_out, dtype=self.attn_dtype, bias=qkv_bias)\n",
    "        self.q_w = nn.Linear(d_in, d_out, dtype=self.attn_dtype, bias=qkv_bias)\n",
    "        self.v_w = nn.Linear(d_in, d_out, dtype=self.attn_dtype, bias=qkv_bias)\n",
    "        self.inverse_d_k = np.reciprocal(np.sqrt(d_out))\n",
    "        \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        weighted_q = self.k_w(x) \n",
    "        weighted_k = self.q_w(x)\n",
    "        attention_scores = weighted_q @ weighted_k.T \n",
    "        attention_weights = torch.softmax(attention_scores*self.inverse_d_k , dim=-1)\n",
    "        weighted_v = self.v_w(x)\n",
    "        context_vectors = attention_weights @ weighted_v\n",
    "        return context_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.5337, -0.1051],\n",
      "        [-0.5323, -0.1080],\n",
      "        [-0.5323, -0.1079],\n",
      "        [-0.5297, -0.1076],\n",
      "        [-0.5311, -0.1066],\n",
      "        [-0.5299, -0.1081]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "sa_v2 = SelfAttentionV2(d_in, d_out)\n",
    "print(sa_v2(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# causal mask\n",
    "\n",
    "지금까지는 입력으로 들어오는 모든 token에 대한 관계를 attention을 통해 계산했다.\n",
    "하지만 그렇게 하기 어려운 경우가 있다. 바로 입력이 아닌 토큰에 대한 예측이다.\n",
    "미래에 나오는 토큰에 대해 attention을 구하는 건 말이 되지 않는다. \n",
    "따라서 미래 토큰을 제외하고 지금있는 token 들에 대해서 attention을 구하는 게 옳다.\n",
    "이를 위해서 도입한게 causla mask다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "context_length=inputs.shape[0]\n",
    "mask_simple = torch.tril(torch.ones(context_length, context_length))\n",
    "print(mask_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 softmax계산을 취한 query 축에 대해서 다 더한 값이 1을 만족해야한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "\n",
    "class CausalAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_in, d_out, attn_dtype: torch.dtype = torch.float32, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_in = d_in\n",
    "        self.d_out = d_out\n",
    "        self.attn_dtype = attn_dtype\n",
    "        self.k_w = nn.Linear(d_in, d_out, dtype=self.attn_dtype, bias=qkv_bias)\n",
    "        self.q_w = nn.Linear(d_in, d_out, dtype=self.attn_dtype, bias=qkv_bias)\n",
    "        self.v_w = nn.Linear(d_in, d_out, dtype=self.attn_dtype, bias=qkv_bias)\n",
    "        self.inverse_d_k = np.reciprocal(np.sqrt(d_out))\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None):\n",
    "        b, context_len, _d_in = x.shape\n",
    "        if mask is None:\n",
    "            self.register_buffer('mask', torch.triu(torch.ones(context_len, context_len), diagonal=1))\n",
    "        \n",
    "        weighted_q = self.k_w(x) \n",
    "        weighted_k = self.q_w(x)\n",
    "        attention_scores = weighted_q @ weighted_k.transpose(1,2)\n",
    "        mask = self.mask.bool().reshape(1, context_len, context_len).tile((b,1,1))\n",
    "        print(f\"mask: {mask}\")\n",
    "        attention_scores.masked_fill_(mask, -torch.inf)\n",
    "        attention_weights = torch.softmax(attention_scores*self.inverse_d_k , dim=-1)\n",
    "        print(f\"attention weight: {attention_weights}\")\n",
    "        weighted_v = self.v_w(x)\n",
    "        context_vectors = attention_weights @ weighted_v\n",
    "        return context_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask: tensor([[[False,  True,  True,  True,  True,  True],\n",
      "         [False, False,  True,  True,  True,  True],\n",
      "         [False, False, False,  True,  True,  True],\n",
      "         [False, False, False, False,  True,  True],\n",
      "         [False, False, False, False, False,  True],\n",
      "         [False, False, False, False, False, False]],\n",
      "\n",
      "        [[False,  True,  True,  True,  True,  True],\n",
      "         [False, False,  True,  True,  True,  True],\n",
      "         [False, False, False,  True,  True,  True],\n",
      "         [False, False, False, False,  True,  True],\n",
      "         [False, False, False, False, False,  True],\n",
      "         [False, False, False, False, False, False]],\n",
      "\n",
      "        [[False,  True,  True,  True,  True,  True],\n",
      "         [False, False,  True,  True,  True,  True],\n",
      "         [False, False, False,  True,  True,  True],\n",
      "         [False, False, False, False,  True,  True],\n",
      "         [False, False, False, False, False,  True],\n",
      "         [False, False, False, False, False, False]]])\n",
      "attention weight: tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000],\n",
      "         [0.2582, 0.2582, 0.2582, 0.2255, 0.0000, 0.0000],\n",
      "         [0.2106, 0.2106, 0.2106, 0.1840, 0.1840, 0.0000],\n",
      "         [0.1779, 0.1779, 0.1779, 0.1554, 0.1554, 0.1554]],\n",
      "\n",
      "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000],\n",
      "         [0.2461, 0.2461, 0.2461, 0.2617, 0.0000, 0.0000],\n",
      "         [0.1950, 0.1950, 0.1950, 0.2074, 0.2074, 0.0000],\n",
      "         [0.1615, 0.1615, 0.1615, 0.1718, 0.1718, 0.1718]],\n",
      "\n",
      "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000],\n",
      "         [0.2587, 0.2587, 0.2587, 0.2239, 0.0000, 0.0000],\n",
      "         [0.2114, 0.2114, 0.2114, 0.1830, 0.1830, 0.0000],\n",
      "         [0.1787, 0.1787, 0.1787, 0.1547, 0.1547, 0.1547]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[-3.2238e-02, -4.6102e-01,  2.8465e-01, -2.2557e-02, -4.8215e-01,\n",
      "           4.3357e-01,  3.7556e-02,  2.4661e-01,  7.1275e-01, -3.8861e-01,\n",
      "          -1.4713e-01,  4.1057e-01],\n",
      "         [-3.2238e-02, -4.6102e-01,  2.8465e-01, -2.2557e-02, -4.8215e-01,\n",
      "           4.3357e-01,  3.7556e-02,  2.4661e-01,  7.1275e-01, -3.8861e-01,\n",
      "          -1.4713e-01,  4.1057e-01],\n",
      "         [-3.2238e-02, -4.6102e-01,  2.8465e-01, -2.2557e-02, -4.8215e-01,\n",
      "           4.3357e-01,  3.7556e-02,  2.4661e-01,  7.1275e-01, -3.8861e-01,\n",
      "          -1.4713e-01,  4.1057e-01],\n",
      "         [-1.3132e-01, -3.9297e-01,  3.3595e-01, -7.1579e-04, -5.0793e-01,\n",
      "           4.8767e-01,  8.8529e-02,  2.1419e-01,  7.6706e-01, -2.9700e-01,\n",
      "          -1.9158e-01,  3.8632e-01],\n",
      "         [-1.9393e-01, -3.4997e-01,  3.6836e-01,  1.3086e-02, -5.2423e-01,\n",
      "           5.2185e-01,  1.2074e-01,  1.9370e-01,  8.0137e-01, -2.3911e-01,\n",
      "          -2.1967e-01,  3.7100e-01],\n",
      "         [-2.3708e-01, -3.2033e-01,  3.9070e-01,  2.2598e-02, -5.3545e-01,\n",
      "           5.4541e-01,  1.4294e-01,  1.7958e-01,  8.2502e-01, -1.9921e-01,\n",
      "          -2.3903e-01,  3.6044e-01]],\n",
      "\n",
      "        [[-4.7519e-01, -1.5250e-01,  4.9789e-01,  7.3523e-02, -5.9142e-01,\n",
      "           6.7262e-01,  2.4216e-01,  1.1294e-01,  9.4776e-01,  1.5378e-02,\n",
      "          -3.3403e-01,  2.8874e-01],\n",
      "         [-4.7519e-01, -1.5250e-01,  4.9789e-01,  7.3523e-02, -5.9142e-01,\n",
      "           6.7262e-01,  2.4216e-01,  1.1294e-01,  9.4776e-01,  1.5378e-02,\n",
      "          -3.3403e-01,  2.8874e-01],\n",
      "         [-4.7519e-01, -1.5250e-01,  4.9789e-01,  7.3523e-02, -5.9142e-01,\n",
      "           6.7262e-01,  2.4216e-01,  1.1294e-01,  9.4776e-01,  1.5378e-02,\n",
      "          -3.3403e-01,  2.8874e-01],\n",
      "         [-4.2431e-01, -1.2464e-01,  4.5054e-01,  6.8005e-02, -5.1839e-01,\n",
      "           5.8986e-01,  2.3655e-01,  8.2472e-02,  8.3111e-01,  3.2089e-02,\n",
      "          -3.0471e-01,  2.5660e-01],\n",
      "         [-3.9453e-01, -1.0833e-01,  4.2283e-01,  6.4776e-02, -4.7566e-01,\n",
      "           5.4143e-01,  2.3327e-01,  6.4644e-02,  7.6286e-01,  4.1868e-02,\n",
      "          -2.8755e-01,  2.3779e-01],\n",
      "         [-3.7498e-01, -9.7634e-02,  4.0465e-01,  6.2657e-02, -4.4762e-01,\n",
      "           5.0964e-01,  2.3111e-01,  5.2942e-02,  7.1805e-01,  4.8286e-02,\n",
      "          -2.7629e-01,  2.2544e-01]],\n",
      "\n",
      "        [[-4.0718e-01,  1.2737e-02,  1.0207e-01,  3.9147e-02, -3.3414e-01,\n",
      "           4.6887e-01, -2.1144e-01,  2.6300e-01,  5.7745e-01, -2.8603e-02,\n",
      "          -5.6285e-02, -5.0504e-02],\n",
      "         [-4.0718e-01,  1.2737e-02,  1.0207e-01,  3.9147e-02, -3.3414e-01,\n",
      "           4.6887e-01, -2.1144e-01,  2.6300e-01,  5.7745e-01, -2.8603e-02,\n",
      "          -5.6285e-02, -5.0504e-02],\n",
      "         [-4.0718e-01,  1.2737e-02,  1.0207e-01,  3.9147e-02, -3.3414e-01,\n",
      "           4.6887e-01, -2.1144e-01,  2.6300e-01,  5.7745e-01, -2.8603e-02,\n",
      "          -5.6285e-02, -5.0504e-02],\n",
      "         [-3.7443e-01, -1.7056e-02,  1.8476e-01,  4.4221e-02, -3.4526e-01,\n",
      "           4.5103e-01, -6.6934e-02,  1.8296e-01,  5.8106e-01, -4.4734e-04,\n",
      "          -1.1795e-01,  3.1610e-02],\n",
      "         [-3.5365e-01, -3.5946e-02,  2.3720e-01,  4.7438e-02, -3.5231e-01,\n",
      "           4.3972e-01,  2.4688e-02,  1.3221e-01,  5.8334e-01,  1.7405e-02,\n",
      "          -1.5704e-01,  8.3676e-02],\n",
      "         [-3.3931e-01, -4.8993e-02,  2.7341e-01,  4.9660e-02, -3.5718e-01,\n",
      "           4.3190e-01,  8.7969e-02,  9.7160e-02,  5.8492e-01,  2.9735e-02,\n",
      "          -1.8404e-01,  1.1964e-01]]], grad_fn=<UnsafeViewBackward0>)\n",
      "context_vecs.shape: torch.Size([3, 6, 12])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "batch_size = 3\n",
    "context_len, embedding = inputs.shape\n",
    "\n",
    "d_out = 12\n",
    "ca = CausalAttention(embedding, d_out)\n",
    "\n",
    "batched_input = inputs.tile((batch_size,)).reshape(-1, context_len, embedding)\n",
    "context_vecs = ca(batched_input)\n",
    "\n",
    "print(context_vecs)\n",
    "print(f\"context_vecs.shape: {context_vecs.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi head attention\n",
    "\n",
    "간단하게 attention을 여러개로 만들어서 해보자는 의미에서 나온 개념"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_in, d_out, num_heads, attn_dtype: torch.dtype = torch.float32, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_in = d_in\n",
    "        self.d_out = d_out\n",
    "        self.attn_dtype = attn_dtype\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "        self.inverse_d_k = np.reciprocal(np.sqrt(d_out))\n",
    "        \n",
    "        self.k_w = nn.Linear(d_in, d_out, dtype=self.attn_dtype, bias=qkv_bias)\n",
    "        self.q_w = nn.Linear(d_in, d_out, dtype=self.attn_dtype, bias=qkv_bias)\n",
    "        self.v_w = nn.Linear(d_in, d_out, dtype=self.attn_dtype, bias=qkv_bias)\n",
    "        \n",
    "        self.out_proj = nn.Linear(d_out, d_out, dtype=self.attn_dtype)\n",
    "        \n",
    "        \n",
    "    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None):\n",
    "        b, context_len, _d_in = x.shape\n",
    "        if mask is None:\n",
    "            self.register_buffer('mask', torch.triu(torch.ones(context_len, context_len), diagonal=1))\n",
    "        \n",
    "        weighted_q = self.k_w(x) \n",
    "        weighted_k = self.q_w(x)\n",
    "        weighted_v = self.v_w(x)\n",
    "        \n",
    "        # head view\n",
    "        # [b, context_len, d_out] -> [b, num_heads, context_len, head_dim]\n",
    "        keys = weighted_k.view(b, context_len, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        querys = weighted_q.view(b, context_len, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        values = weighted_v.view(b, context_len, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        \n",
    "        attention_scores = querys @ keys.transpose(2,3)\n",
    "        mask = self.mask.bool().reshape(1, 1, context_len, context_len).tile((b,1,1,1))\n",
    "        print(f\"mask: {mask}\")\n",
    "        attention_scores.masked_fill_(mask, -torch.inf)\n",
    "        attention_weights = torch.softmax(attention_scores*self.inverse_d_k , dim=-1)\n",
    "        print(f\"attention weight: {attention_weights}\")\n",
    "        \n",
    "        # [b, context_len, num_heads, head_dim]\n",
    "        context_vectors = (attention_weights @ values).transpose(1,2)\n",
    "        context_vectors = context_vectors.contiguous().view(b, context_len, self.d_out)\n",
    "        context_vectors = self.out_proj(context_vectors)\n",
    "        \n",
    "        return context_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask: tensor([[[[False,  True,  True,  True,  True,  True],\n",
      "          [False, False,  True,  True,  True,  True],\n",
      "          [False, False, False,  True,  True,  True],\n",
      "          [False, False, False, False,  True,  True],\n",
      "          [False, False, False, False, False,  True],\n",
      "          [False, False, False, False, False, False]]],\n",
      "\n",
      "\n",
      "        [[[False,  True,  True,  True,  True,  True],\n",
      "          [False, False,  True,  True,  True,  True],\n",
      "          [False, False, False,  True,  True,  True],\n",
      "          [False, False, False, False,  True,  True],\n",
      "          [False, False, False, False, False,  True],\n",
      "          [False, False, False, False, False, False]]],\n",
      "\n",
      "\n",
      "        [[[False,  True,  True,  True,  True,  True],\n",
      "          [False, False,  True,  True,  True,  True],\n",
      "          [False, False, False,  True,  True,  True],\n",
      "          [False, False, False, False,  True,  True],\n",
      "          [False, False, False, False, False,  True],\n",
      "          [False, False, False, False, False, False]]]])\n",
      "attention weight: tensor([[[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000],\n",
      "          [0.2557, 0.2557, 0.2557, 0.2330, 0.0000, 0.0000],\n",
      "          [0.2073, 0.2073, 0.2073, 0.1890, 0.1890, 0.0000],\n",
      "          [0.1744, 0.1744, 0.1744, 0.1589, 0.1589, 0.1589]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000],\n",
      "          [0.2538, 0.2538, 0.2538, 0.2387, 0.0000, 0.0000],\n",
      "          [0.2049, 0.2049, 0.2049, 0.1927, 0.1927, 0.0000],\n",
      "          [0.1718, 0.1718, 0.1718, 0.1616, 0.1616, 0.1616]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000],\n",
      "          [0.2514, 0.2514, 0.2514, 0.2459, 0.0000, 0.0000],\n",
      "          [0.2018, 0.2018, 0.2018, 0.1973, 0.1973, 0.0000],\n",
      "          [0.1685, 0.1685, 0.1685, 0.1648, 0.1648, 0.1648]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000],\n",
      "          [0.2474, 0.2474, 0.2474, 0.2578, 0.0000, 0.0000],\n",
      "          [0.1967, 0.1967, 0.1967, 0.2050, 0.2050, 0.0000],\n",
      "          [0.1632, 0.1632, 0.1632, 0.1701, 0.1701, 0.1701]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000],\n",
      "          [0.2500, 0.2500, 0.2500, 0.2499, 0.0000, 0.0000],\n",
      "          [0.2000, 0.2000, 0.2000, 0.1999, 0.1999, 0.0000],\n",
      "          [0.1667, 0.1667, 0.1667, 0.1666, 0.1666, 0.1666]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000],\n",
      "          [0.2472, 0.2472, 0.2472, 0.2583, 0.0000, 0.0000],\n",
      "          [0.1965, 0.1965, 0.1965, 0.2053, 0.2053, 0.0000],\n",
      "          [0.1630, 0.1630, 0.1630, 0.1703, 0.1703, 0.1703]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000],\n",
      "          [0.2491, 0.2491, 0.2491, 0.2526, 0.0000, 0.0000],\n",
      "          [0.1989, 0.1989, 0.1989, 0.2017, 0.2017, 0.0000],\n",
      "          [0.1655, 0.1655, 0.1655, 0.1678, 0.1678, 0.1678]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000],\n",
      "          [0.2497, 0.2497, 0.2497, 0.2508, 0.0000, 0.0000],\n",
      "          [0.1997, 0.1997, 0.1997, 0.2005, 0.2005, 0.0000],\n",
      "          [0.1663, 0.1663, 0.1663, 0.1670, 0.1670, 0.1670]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000],\n",
      "          [0.2529, 0.2529, 0.2529, 0.2412, 0.0000, 0.0000],\n",
      "          [0.2038, 0.2038, 0.2038, 0.1943, 0.1943, 0.0000],\n",
      "          [0.1706, 0.1706, 0.1706, 0.1627, 0.1627, 0.1627]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000],\n",
      "          [0.2496, 0.2496, 0.2496, 0.2513, 0.0000, 0.0000],\n",
      "          [0.1995, 0.1995, 0.1995, 0.2008, 0.2008, 0.0000],\n",
      "          [0.1661, 0.1661, 0.1661, 0.1672, 0.1672, 0.1672]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000],\n",
      "          [0.2568, 0.2568, 0.2568, 0.2297, 0.0000, 0.0000],\n",
      "          [0.2088, 0.2088, 0.2088, 0.1868, 0.1868, 0.0000],\n",
      "          [0.1759, 0.1759, 0.1759, 0.1574, 0.1574, 0.1574]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000],\n",
      "          [0.2495, 0.2495, 0.2495, 0.2515, 0.0000, 0.0000],\n",
      "          [0.1993, 0.1993, 0.1993, 0.2010, 0.2010, 0.0000],\n",
      "          [0.1660, 0.1660, 0.1660, 0.1674, 0.1674, 0.1674]]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[ 0.1459, -0.3421,  0.1385,  0.0031,  0.2925, -0.5214,  0.5882,\n",
      "          -0.1075,  0.0167,  0.2416,  0.1121,  0.2292],\n",
      "         [ 0.1459, -0.3421,  0.1385,  0.0031,  0.2925, -0.5214,  0.5882,\n",
      "          -0.1075,  0.0167,  0.2416,  0.1121,  0.2292],\n",
      "         [ 0.1459, -0.3421,  0.1385,  0.0031,  0.2925, -0.5214,  0.5882,\n",
      "          -0.1075,  0.0167,  0.2416,  0.1121,  0.2292],\n",
      "         [ 0.1077, -0.2933,  0.0963, -0.0225,  0.3272, -0.5751,  0.5835,\n",
      "          -0.1239,  0.0343,  0.2152,  0.1297,  0.2239],\n",
      "         [ 0.0845, -0.2638,  0.0706, -0.0385,  0.3483, -0.6073,  0.5810,\n",
      "          -0.1341,  0.0457,  0.1990,  0.1401,  0.2202],\n",
      "         [ 0.0690, -0.2440,  0.0534, -0.0495,  0.3625, -0.6288,  0.5794,\n",
      "          -0.1410,  0.0538,  0.1881,  0.1469,  0.2174]],\n",
      "\n",
      "        [[-0.0125, -0.1502, -0.0385, -0.1075,  0.4406, -0.7293,  0.5758,\n",
      "          -0.1743,  0.1021,  0.1267,  0.1711,  0.2088],\n",
      "         [-0.0125, -0.1502, -0.0385, -0.1075,  0.4406, -0.7293,  0.5758,\n",
      "          -0.1743,  0.1021,  0.1267,  0.1711,  0.2088],\n",
      "         [-0.0125, -0.1502, -0.0385, -0.1075,  0.4406, -0.7293,  0.5758,\n",
      "          -0.1743,  0.1021,  0.1267,  0.1711,  0.2088],\n",
      "         [ 0.0187, -0.1477, -0.0384, -0.0976,  0.3883, -0.6758,  0.5311,\n",
      "          -0.1182,  0.0619,  0.1104,  0.1371,  0.2093],\n",
      "         [ 0.0372, -0.1466, -0.0384, -0.0917,  0.3573, -0.6438,  0.5045,\n",
      "          -0.0849,  0.0381,  0.1006,  0.1170,  0.2097],\n",
      "         [ 0.0495, -0.1459, -0.0385, -0.0878,  0.3367, -0.6225,  0.4868,\n",
      "          -0.0627,  0.0223,  0.0942,  0.1036,  0.2101]],\n",
      "\n",
      "        [[ 0.0163, -0.2881, -0.1070, -0.0576,  0.4365, -0.4531,  0.5381,\n",
      "           0.0142,  0.0758,  0.0154, -0.0776,  0.3792],\n",
      "         [ 0.0163, -0.2881, -0.1070, -0.0576,  0.4365, -0.4531,  0.5381,\n",
      "           0.0142,  0.0758,  0.0154, -0.0776,  0.3792],\n",
      "         [ 0.0163, -0.2881, -0.1070, -0.0576,  0.4365, -0.4531,  0.5381,\n",
      "           0.0142,  0.0758,  0.0154, -0.0776,  0.3792],\n",
      "         [ 0.0435, -0.2380, -0.0808, -0.0613,  0.3813, -0.4898,  0.5024,\n",
      "           0.0080,  0.0390,  0.0364, -0.0258,  0.3188],\n",
      "         [ 0.0598, -0.2075, -0.0647, -0.0642,  0.3465, -0.5123,  0.4803,\n",
      "           0.0048,  0.0166,  0.0497,  0.0056,  0.2819],\n",
      "         [ 0.0706, -0.1870, -0.0539, -0.0665,  0.3225, -0.5275,  0.4652,\n",
      "           0.0029,  0.0015,  0.0590,  0.0267,  0.2571]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "context_vecs.shape: torch.Size([3, 6, 12])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "batch_size = 3\n",
    "context_len, embedding = inputs.shape\n",
    "\n",
    "d_out = 12\n",
    "num_heads = 4\n",
    "ca = MultiHeadAttention(embedding, d_out, num_heads)\n",
    "\n",
    "batched_input = inputs.tile((batch_size,)).reshape(-1, context_len, embedding)\n",
    "context_vecs = ca(batched_input)\n",
    "\n",
    "print(context_vecs)\n",
    "print(f\"context_vecs.shape: {context_vecs.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
